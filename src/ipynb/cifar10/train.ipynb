{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "BASE_DIR = '../../../'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# custom code\n",
    "import utils.utils\n",
    "CONFIG = utils.utils.load_config(\"../../config.json\")\n",
    "import utils.custom_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 asym 0.6\n"
     ]
    }
   ],
   "source": [
    "DATASET = os.path.basename(os.getcwd()) # name of folder this file is in\n",
    "RANDOM_SEED = CONFIG['random_seed']\n",
    "# type of noise\n",
    "# asym: classes flip to a single other class\n",
    "# sym: classes flip uniformly to any other class\n",
    "TYPE = CONFIG[\"experiment_configs\"][DATASET][\"type\"]\n",
    " # chance of flip\n",
    "NOISE_P = CONFIG[\"experiment_configs\"][DATASET][\"noise_p\"]\n",
    "\n",
    "EPOCHS = CONFIG[\"experiment_configs\"][DATASET][\"epochs\"]\n",
    "BATCH_SIZE = CONFIG[\"experiment_configs\"][DATASET][\"batch_size\"]\n",
    "IMAGE_X = CONFIG[\"experiment_configs\"][DATASET][\"image_x_size\"]\n",
    "IMAGE_Y = CONFIG[\"experiment_configs\"][DATASET][\"image_y_size\"]\n",
    "IMAGE_SIZE = (IMAGE_Y, IMAGE_X)\n",
    "\n",
    "print(RANDOM_SEED, TYPE, NOISE_P)\n",
    "\n",
    "# folders for processed, models\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, f'processed/{DATASET}/rs={RANDOM_SEED}')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, f'models/{DATASET}/rs={RANDOM_SEED}')\n",
    "\n",
    "PROCESSED_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, DATASET, \".npz\", t=TYPE, np=NOISE_P)\n",
    "# mt = model_type\n",
    "BASE_MODEL_SAVEPATH = utils.utils.get_savepath(MODELS_DIR, DATASET, \".h5\", mt=\"base\", t=TYPE, np=NOISE_P)\n",
    "\n",
    "if os.path.exists(BASE_MODEL_SAVEPATH):\n",
    "    print(f\"warning: model has been run for rs={RANDOM_SEED}_t={TYPE}_np={NOISE_P}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_SAVEPATH, 'rb') as f:\n",
    "    dat = np.load(f)\n",
    "\n",
    "    x_train = dat['x_train']\n",
    "    y_train = dat['y_train']\n",
    "\n",
    "    x_hyper_train = dat['x_hyper_train']\n",
    "    y_hyper_train = dat['y_hyper_train']\n",
    "    \n",
    "    x_val = dat['x_val']\n",
    "    y_val = dat['y_val']\n",
    "    \n",
    "    x_hyper_val = dat['x_hyper_val']\n",
    "    y_hyper_val = dat['y_hyper_val']\n",
    "\n",
    "    x_test = dat['x_test']\n",
    "    y_test = dat['y_test']\n",
    "\n",
    "x_val_full = np.concatenate([x_val, x_hyper_val])\n",
    "y_val_full = np.concatenate([y_val, y_hyper_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_hyper_train = tf.keras.utils.to_categorical(y_hyper_train)\n",
    "y_val_full = tf.keras.utils.to_categorical(y_val_full)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.utils.make_resnet(\n",
    "    depth=2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    input_shape=(*IMAGE_SIZE,3),\n",
    "    nc=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           add[0][0]                        \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 32)   4640        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   544         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 32)   9248        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 32)   0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 32)   128         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 16, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9248        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9248        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 64)     18496       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 64)     256         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 8, 8, 64)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 64)     2112        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 64)     36928       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 64)     0           conv2d_12[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 8, 8, 64)     256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 8, 8, 64)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 64)     36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 8, 8, 64)     256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 8, 8, 64)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 64)     0           add_4[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 64)     256         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 8, 8, 64)     0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 176,554\n",
      "Trainable params: 175,626\n",
      "Non-trainable params: 928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer, loss='categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will periodically show the accuracy on valid and test\n",
    "show_val_test = utils.custom_tf.ShowValidAndTest(\n",
    "    (x_val_full, y_val_full),\n",
    "    (x_test, y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epoch_freq=5,\n",
    ")\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch > 80:\n",
    "        return 0.001\n",
    "    elif epoch > 40:\n",
    "        return 0.01\n",
    "    else:\n",
    "        return 0.1\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# save best model every epoch\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=BASE_MODEL_SAVEPATH,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "callbacks = [lr_scheduler, show_val_test, save_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.7624 - accuracy: 0.3996\n",
      "\n",
      "valid: 0.2865\n",
      "test: 0.2995\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.37777, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 26s 87ms/step - loss: 1.7624 - accuracy: 0.3996 - val_loss: 2.3778 - val_accuracy: 0.3327\n",
      "Epoch 2/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.4263 - accuracy: 0.5226\n",
      "Epoch 00002: val_loss improved from 2.37777 to 2.26452, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.4263 - accuracy: 0.5226 - val_loss: 2.2645 - val_accuracy: 0.4480\n",
      "Epoch 3/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.2793 - accuracy: 0.5709\n",
      "Epoch 00003: val_loss improved from 2.26452 to 1.71165, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.2793 - accuracy: 0.5709 - val_loss: 1.7116 - val_accuracy: 0.4606\n",
      "Epoch 4/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.1927 - accuracy: 0.5948\n",
      "Epoch 00004: val_loss improved from 1.71165 to 1.33839, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.1927 - accuracy: 0.5948 - val_loss: 1.3384 - val_accuracy: 0.5538\n",
      "Epoch 5/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.1343 - accuracy: 0.6086\n",
      "Epoch 00005: val_loss improved from 1.33839 to 1.20974, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.1343 - accuracy: 0.6086 - val_loss: 1.2097 - val_accuracy: 0.5936\n",
      "Epoch 6/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.6238\n",
      "\n",
      "valid: 0.3655\n",
      "test: 0.3536\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.20974\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 1.0912 - accuracy: 0.6238 - val_loss: 1.7739 - val_accuracy: 0.4898\n",
      "Epoch 7/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.6331\n",
      "Epoch 00007: val_loss did not improve from 1.20974\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.0530 - accuracy: 0.6331 - val_loss: 1.5878 - val_accuracy: 0.5450\n",
      "Epoch 8/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.0216 - accuracy: 0.6409\n",
      "Epoch 00008: val_loss did not improve from 1.20974\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 1.0216 - accuracy: 0.6409 - val_loss: 1.2804 - val_accuracy: 0.6016\n",
      "Epoch 9/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9896 - accuracy: 0.6539\n",
      "Epoch 00009: val_loss improved from 1.20974 to 1.06873, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.9896 - accuracy: 0.6539 - val_loss: 1.0687 - val_accuracy: 0.6398\n",
      "Epoch 10/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9738 - accuracy: 0.6540\n",
      "Epoch 00010: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.9738 - accuracy: 0.6540 - val_loss: 1.4949 - val_accuracy: 0.5522\n",
      "Epoch 11/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9504 - accuracy: 0.6623\n",
      "\n",
      "valid: 0.498\n",
      "test: 0.4905\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 25s 85ms/step - loss: 0.9504 - accuracy: 0.6623 - val_loss: 1.3348 - val_accuracy: 0.5735\n",
      "Epoch 12/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9322 - accuracy: 0.6667\n",
      "Epoch 00012: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.9322 - accuracy: 0.6667 - val_loss: 1.2191 - val_accuracy: 0.5934\n",
      "Epoch 13/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9247 - accuracy: 0.6693\n",
      "Epoch 00013: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.9247 - accuracy: 0.6693 - val_loss: 1.0891 - val_accuracy: 0.6442\n",
      "Epoch 14/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.6766\n",
      "Epoch 00014: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.9034 - accuracy: 0.6766 - val_loss: 1.6197 - val_accuracy: 0.5356\n",
      "Epoch 15/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8941 - accuracy: 0.6791\n",
      "Epoch 00015: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8941 - accuracy: 0.6791 - val_loss: 1.3400 - val_accuracy: 0.5713\n",
      "Epoch 16/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8868 - accuracy: 0.6826\n",
      "\n",
      "valid: 0.5275\n",
      "test: 0.5319\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.06873\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.8868 - accuracy: 0.6826 - val_loss: 1.2566 - val_accuracy: 0.6145\n",
      "Epoch 17/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8694 - accuracy: 0.6867\n",
      "Epoch 00017: val_loss improved from 1.06873 to 0.96706, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8694 - accuracy: 0.6867 - val_loss: 0.9671 - val_accuracy: 0.6689\n",
      "Epoch 18/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8640 - accuracy: 0.6894\n",
      "Epoch 00018: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8640 - accuracy: 0.6894 - val_loss: 1.0779 - val_accuracy: 0.6287\n",
      "Epoch 19/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8603 - accuracy: 0.6905\n",
      "Epoch 00019: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8603 - accuracy: 0.6905 - val_loss: 1.0399 - val_accuracy: 0.6460\n",
      "Epoch 20/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8546 - accuracy: 0.6902\n",
      "Epoch 00020: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8546 - accuracy: 0.6902 - val_loss: 1.1270 - val_accuracy: 0.6359\n",
      "Epoch 21/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8522 - accuracy: 0.6938\n",
      "\n",
      "valid: 0.5095\n",
      "test: 0.513\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.8522 - accuracy: 0.6938 - val_loss: 1.2541 - val_accuracy: 0.6130\n",
      "Epoch 22/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8454 - accuracy: 0.6943\n",
      "Epoch 00022: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8454 - accuracy: 0.6943 - val_loss: 1.1662 - val_accuracy: 0.6068\n",
      "Epoch 23/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8381 - accuracy: 0.6961\n",
      "Epoch 00023: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8381 - accuracy: 0.6961 - val_loss: 1.0677 - val_accuracy: 0.6460\n",
      "Epoch 24/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8327 - accuracy: 0.7003\n",
      "Epoch 00024: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8327 - accuracy: 0.7003 - val_loss: 1.1593 - val_accuracy: 0.6222\n",
      "Epoch 25/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8265 - accuracy: 0.7019\n",
      "Epoch 00025: val_loss did not improve from 0.96706\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8265 - accuracy: 0.7019 - val_loss: 1.1528 - val_accuracy: 0.6321\n",
      "Epoch 26/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.7014\n",
      "\n",
      "valid: 0.6075\n",
      "test: 0.6053\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.96706 to 0.96654, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 25s 85ms/step - loss: 0.8264 - accuracy: 0.7014 - val_loss: 0.9665 - val_accuracy: 0.6465\n",
      "Epoch 27/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8232 - accuracy: 0.7036\n",
      "Epoch 00027: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8232 - accuracy: 0.7036 - val_loss: 1.0949 - val_accuracy: 0.6256\n",
      "Epoch 28/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8207 - accuracy: 0.7052\n",
      "Epoch 00028: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8207 - accuracy: 0.7052 - val_loss: 0.9873 - val_accuracy: 0.6644\n",
      "Epoch 29/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.7052\n",
      "Epoch 00029: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.8167 - accuracy: 0.7052 - val_loss: 1.0755 - val_accuracy: 0.6319\n",
      "Epoch 30/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.7056\n",
      "Epoch 00030: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.8093 - accuracy: 0.7056 - val_loss: 1.0818 - val_accuracy: 0.6630\n",
      "Epoch 31/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8025 - accuracy: 0.7101\n",
      "\n",
      "valid: 0.5405\n",
      "test: 0.5349\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 25s 85ms/step - loss: 0.8025 - accuracy: 0.7101 - val_loss: 1.0097 - val_accuracy: 0.6523\n",
      "Epoch 32/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.7075\n",
      "Epoch 00032: val_loss did not improve from 0.96654\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8085 - accuracy: 0.7075 - val_loss: 1.0730 - val_accuracy: 0.6345\n",
      "Epoch 33/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8030 - accuracy: 0.7131\n",
      "Epoch 00033: val_loss improved from 0.96654 to 0.90505, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.8030 - accuracy: 0.7131 - val_loss: 0.9050 - val_accuracy: 0.6761\n",
      "Epoch 34/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7988 - accuracy: 0.7123\n",
      "Epoch 00034: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.7988 - accuracy: 0.7123 - val_loss: 1.0630 - val_accuracy: 0.6472\n",
      "Epoch 35/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7998 - accuracy: 0.7084\n",
      "Epoch 00035: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.7998 - accuracy: 0.7084 - val_loss: 0.9679 - val_accuracy: 0.6622\n",
      "Epoch 36/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7942 - accuracy: 0.7102\n",
      "\n",
      "valid: 0.5225\n",
      "test: 0.5172\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.7942 - accuracy: 0.7102 - val_loss: 1.0841 - val_accuracy: 0.6489\n",
      "Epoch 37/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7985 - accuracy: 0.7099\n",
      "Epoch 00037: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.7985 - accuracy: 0.7099 - val_loss: 1.2160 - val_accuracy: 0.6230\n",
      "Epoch 38/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7891 - accuracy: 0.7125\n",
      "Epoch 00038: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.7891 - accuracy: 0.7125 - val_loss: 1.0067 - val_accuracy: 0.6588\n",
      "Epoch 39/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7888 - accuracy: 0.7125\n",
      "Epoch 00039: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.7888 - accuracy: 0.7125 - val_loss: 1.0724 - val_accuracy: 0.6496\n",
      "Epoch 40/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7915 - accuracy: 0.7147\n",
      "Epoch 00040: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.7915 - accuracy: 0.7147 - val_loss: 1.5744 - val_accuracy: 0.5857\n",
      "Epoch 41/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7862 - accuracy: 0.7128\n",
      "\n",
      "valid: 0.489\n",
      "test: 0.4996\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.90505\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.7862 - accuracy: 0.7128 - val_loss: 0.9497 - val_accuracy: 0.6820\n",
      "Epoch 42/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7175 - accuracy: 0.7407\n",
      "Epoch 00042: val_loss improved from 0.90505 to 0.80291, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.7175 - accuracy: 0.7407 - val_loss: 0.8029 - val_accuracy: 0.7086\n",
      "Epoch 43/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6715 - accuracy: 0.7557\n",
      "Epoch 00043: val_loss improved from 0.80291 to 0.76869, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6715 - accuracy: 0.7557 - val_loss: 0.7687 - val_accuracy: 0.7235\n",
      "Epoch 44/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6580 - accuracy: 0.7562\n",
      "Epoch 00044: val_loss improved from 0.76869 to 0.76408, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6580 - accuracy: 0.7562 - val_loss: 0.7641 - val_accuracy: 0.7257\n",
      "Epoch 45/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6495 - accuracy: 0.7584\n",
      "Epoch 00045: val_loss improved from 0.76408 to 0.75075, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6495 - accuracy: 0.7584 - val_loss: 0.7508 - val_accuracy: 0.7189\n",
      "Epoch 46/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.7610\n",
      "\n",
      "valid: 0.529\n",
      "test: 0.5338\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.75075\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.6418 - accuracy: 0.7610 - val_loss: 0.7547 - val_accuracy: 0.7258\n",
      "Epoch 47/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.7629\n",
      "Epoch 00047: val_loss did not improve from 0.75075\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.6354 - accuracy: 0.7629 - val_loss: 0.7608 - val_accuracy: 0.7220\n",
      "Epoch 48/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6311 - accuracy: 0.7611\n",
      "Epoch 00048: val_loss did not improve from 0.75075\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.6311 - accuracy: 0.7611 - val_loss: 0.7745 - val_accuracy: 0.7172\n",
      "Epoch 49/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6228 - accuracy: 0.7628\n",
      "Epoch 00049: val_loss did not improve from 0.75075\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6228 - accuracy: 0.7628 - val_loss: 0.7845 - val_accuracy: 0.7094\n",
      "Epoch 50/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6174 - accuracy: 0.7654\n",
      "Epoch 00050: val_loss did not improve from 0.75075\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6174 - accuracy: 0.7654 - val_loss: 0.7583 - val_accuracy: 0.7194\n",
      "Epoch 51/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6112 - accuracy: 0.7654\n",
      "\n",
      "valid: 0.538\n",
      "test: 0.5398\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.75075 to 0.74097, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.6112 - accuracy: 0.7654 - val_loss: 0.7410 - val_accuracy: 0.7265\n",
      "Epoch 52/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7681\n",
      "Epoch 00052: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.6079 - accuracy: 0.7681 - val_loss: 0.7654 - val_accuracy: 0.7124\n",
      "Epoch 53/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6055 - accuracy: 0.7679\n",
      "Epoch 00053: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.6055 - accuracy: 0.7679 - val_loss: 0.7684 - val_accuracy: 0.7206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6071 - accuracy: 0.7664\n",
      "Epoch 00054: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.6071 - accuracy: 0.7664 - val_loss: 0.7614 - val_accuracy: 0.7195\n",
      "Epoch 55/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5983 - accuracy: 0.7686\n",
      "Epoch 00055: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5983 - accuracy: 0.7686 - val_loss: 0.7689 - val_accuracy: 0.7216\n",
      "Epoch 56/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6005 - accuracy: 0.7681\n",
      "\n",
      "valid: 0.5745\n",
      "test: 0.575\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.6005 - accuracy: 0.7681 - val_loss: 0.7777 - val_accuracy: 0.7149\n",
      "Epoch 57/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5910 - accuracy: 0.7714\n",
      "Epoch 00057: val_loss did not improve from 0.74097\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5910 - accuracy: 0.7714 - val_loss: 0.7781 - val_accuracy: 0.7236\n",
      "Epoch 58/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5883 - accuracy: 0.7702\n",
      "Epoch 00058: val_loss improved from 0.74097 to 0.74023, saving model to ../../../models/cifar10/rs=25/cifar10_mt=base_np=0.6_t=asym.h5\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5883 - accuracy: 0.7702 - val_loss: 0.7402 - val_accuracy: 0.7271\n",
      "Epoch 59/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.7746\n",
      "Epoch 00059: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5823 - accuracy: 0.7746 - val_loss: 0.7519 - val_accuracy: 0.7156\n",
      "Epoch 60/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5804 - accuracy: 0.7756\n",
      "Epoch 00060: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5804 - accuracy: 0.7756 - val_loss: 0.7819 - val_accuracy: 0.7161\n",
      "Epoch 61/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5795 - accuracy: 0.7760\n",
      "\n",
      "valid: 0.56\n",
      "test: 0.5611\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.5795 - accuracy: 0.7760 - val_loss: 0.7568 - val_accuracy: 0.7197\n",
      "Epoch 62/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.7766\n",
      "Epoch 00062: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5728 - accuracy: 0.7766 - val_loss: 0.7907 - val_accuracy: 0.7092\n",
      "Epoch 63/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5710 - accuracy: 0.7751\n",
      "Epoch 00063: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5710 - accuracy: 0.7751 - val_loss: 0.7505 - val_accuracy: 0.7161\n",
      "Epoch 64/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5683 - accuracy: 0.7763\n",
      "Epoch 00064: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5683 - accuracy: 0.7763 - val_loss: 0.7783 - val_accuracy: 0.7122\n",
      "Epoch 65/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.7760\n",
      "Epoch 00065: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5696 - accuracy: 0.7760 - val_loss: 0.7719 - val_accuracy: 0.7097\n",
      "Epoch 66/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5656 - accuracy: 0.7763\n",
      "\n",
      "valid: 0.5415\n",
      "test: 0.5502\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.5656 - accuracy: 0.7763 - val_loss: 0.7692 - val_accuracy: 0.7165\n",
      "Epoch 67/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7809\n",
      "Epoch 00067: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5610 - accuracy: 0.7809 - val_loss: 0.7638 - val_accuracy: 0.7184\n",
      "Epoch 68/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.7779\n",
      "Epoch 00068: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.5608 - accuracy: 0.7779 - val_loss: 0.7639 - val_accuracy: 0.7093\n",
      "Epoch 69/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5591 - accuracy: 0.7783\n",
      "Epoch 00069: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5591 - accuracy: 0.7783 - val_loss: 0.7740 - val_accuracy: 0.7109\n",
      "Epoch 70/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5558 - accuracy: 0.7806\n",
      "Epoch 00070: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5558 - accuracy: 0.7806 - val_loss: 0.7814 - val_accuracy: 0.7209\n",
      "Epoch 71/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5524 - accuracy: 0.7813\n",
      "\n",
      "valid: 0.5815\n",
      "test: 0.578\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.5524 - accuracy: 0.7813 - val_loss: 0.8001 - val_accuracy: 0.7099\n",
      "Epoch 72/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5522 - accuracy: 0.7813\n",
      "Epoch 00072: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5522 - accuracy: 0.7813 - val_loss: 0.7586 - val_accuracy: 0.7144\n",
      "Epoch 73/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.7825\n",
      "Epoch 00073: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5483 - accuracy: 0.7825 - val_loss: 0.7936 - val_accuracy: 0.7090\n",
      "Epoch 74/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5448 - accuracy: 0.7855\n",
      "Epoch 00074: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5448 - accuracy: 0.7855 - val_loss: 0.8106 - val_accuracy: 0.7103\n",
      "Epoch 75/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5442 - accuracy: 0.7847\n",
      "Epoch 00075: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.5442 - accuracy: 0.7847 - val_loss: 0.7674 - val_accuracy: 0.7103\n",
      "Epoch 76/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5434 - accuracy: 0.7837\n",
      "\n",
      "valid: 0.618\n",
      "test: 0.6214\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.5434 - accuracy: 0.7837 - val_loss: 0.7742 - val_accuracy: 0.7025\n",
      "Epoch 77/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.7856\n",
      "Epoch 00077: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5401 - accuracy: 0.7856 - val_loss: 0.7954 - val_accuracy: 0.7094\n",
      "Epoch 78/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.7868\n",
      "Epoch 00078: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 79ms/step - loss: 0.5384 - accuracy: 0.7868 - val_loss: 0.7789 - val_accuracy: 0.7071\n",
      "Epoch 79/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5362 - accuracy: 0.7850\n",
      "Epoch 00079: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.5362 - accuracy: 0.7850 - val_loss: 0.7718 - val_accuracy: 0.7076\n",
      "Epoch 80/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.7889\n",
      "Epoch 00080: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.5305 - accuracy: 0.7889 - val_loss: 0.8072 - val_accuracy: 0.7092\n",
      "Epoch 81/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5272 - accuracy: 0.7903\n",
      "\n",
      "valid: 0.5865\n",
      "test: 0.5831\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.5272 - accuracy: 0.7903 - val_loss: 0.8176 - val_accuracy: 0.7033\n",
      "Epoch 82/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5194 - accuracy: 0.7959\n",
      "Epoch 00082: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.5194 - accuracy: 0.7959 - val_loss: 0.7457 - val_accuracy: 0.7163\n",
      "Epoch 83/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.7994\n",
      "Epoch 00083: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.5069 - accuracy: 0.7994 - val_loss: 0.7474 - val_accuracy: 0.7158\n",
      "Epoch 84/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7998\n",
      "Epoch 00084: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.5032 - accuracy: 0.7998 - val_loss: 0.7412 - val_accuracy: 0.7174\n",
      "Epoch 85/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5030 - accuracy: 0.8016\n",
      "Epoch 00085: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.5030 - accuracy: 0.8016 - val_loss: 0.7432 - val_accuracy: 0.7190\n",
      "Epoch 86/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.8032\n",
      "\n",
      "valid: 0.587\n",
      "test: 0.5863\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.5001 - accuracy: 0.8032 - val_loss: 0.7416 - val_accuracy: 0.7183\n",
      "Epoch 87/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8057\n",
      "Epoch 00087: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.5000 - accuracy: 0.8057 - val_loss: 0.7421 - val_accuracy: 0.7195\n",
      "Epoch 88/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4992 - accuracy: 0.8049\n",
      "Epoch 00088: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.4992 - accuracy: 0.8049 - val_loss: 0.7447 - val_accuracy: 0.7128\n",
      "Epoch 89/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4990 - accuracy: 0.8030\n",
      "Epoch 00089: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.4990 - accuracy: 0.8030 - val_loss: 0.7424 - val_accuracy: 0.7168\n",
      "Epoch 90/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8052\n",
      "Epoch 00090: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.4982 - accuracy: 0.8052 - val_loss: 0.7479 - val_accuracy: 0.7178\n",
      "Epoch 91/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4952 - accuracy: 0.8056\n",
      "\n",
      "valid: 0.595\n",
      "test: 0.593\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.4952 - accuracy: 0.8056 - val_loss: 0.7453 - val_accuracy: 0.7153\n",
      "Epoch 92/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4971 - accuracy: 0.8039\n",
      "Epoch 00092: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.4971 - accuracy: 0.8039 - val_loss: 0.7509 - val_accuracy: 0.7194\n",
      "Epoch 93/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4966 - accuracy: 0.8033\n",
      "Epoch 00093: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.4966 - accuracy: 0.8033 - val_loss: 0.7518 - val_accuracy: 0.7169\n",
      "Epoch 94/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4935 - accuracy: 0.8060\n",
      "Epoch 00094: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.4935 - accuracy: 0.8060 - val_loss: 0.7464 - val_accuracy: 0.7169\n",
      "Epoch 95/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8071\n",
      "Epoch 00095: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.4947 - accuracy: 0.8071 - val_loss: 0.7511 - val_accuracy: 0.7148\n",
      "Epoch 96/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8078\n",
      "\n",
      "valid: 0.597\n",
      "test: 0.5962\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.4913 - accuracy: 0.8078 - val_loss: 0.7463 - val_accuracy: 0.7173\n",
      "Epoch 97/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4912 - accuracy: 0.8049\n",
      "Epoch 00097: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 79ms/step - loss: 0.4912 - accuracy: 0.8049 - val_loss: 0.7528 - val_accuracy: 0.7159\n",
      "Epoch 98/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4908 - accuracy: 0.8061\n",
      "Epoch 00098: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 77ms/step - loss: 0.4908 - accuracy: 0.8061 - val_loss: 0.7511 - val_accuracy: 0.7147\n",
      "Epoch 99/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8040\n",
      "Epoch 00099: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 77ms/step - loss: 0.4913 - accuracy: 0.8040 - val_loss: 0.7456 - val_accuracy: 0.7159\n",
      "Epoch 100/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.8049\n",
      "Epoch 00100: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 23s 78ms/step - loss: 0.4922 - accuracy: 0.8049 - val_loss: 0.7524 - val_accuracy: 0.7155\n",
      "Epoch 101/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.8075\n",
      "\n",
      "valid: 0.591\n",
      "test: 0.5913\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.4911 - accuracy: 0.8075 - val_loss: 0.7544 - val_accuracy: 0.7160\n",
      "Epoch 102/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.8073\n",
      "Epoch 00102: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4900 - accuracy: 0.8073 - val_loss: 0.7499 - val_accuracy: 0.7165\n",
      "Epoch 103/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4894 - accuracy: 0.8072\n",
      "Epoch 00103: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4894 - accuracy: 0.8072 - val_loss: 0.7512 - val_accuracy: 0.7163\n",
      "Epoch 104/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.8102\n",
      "Epoch 00104: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4866 - accuracy: 0.8102 - val_loss: 0.7571 - val_accuracy: 0.7148\n",
      "Epoch 105/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4891 - accuracy: 0.8076\n",
      "Epoch 00105: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4891 - accuracy: 0.8076 - val_loss: 0.7543 - val_accuracy: 0.7147\n",
      "Epoch 106/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4870 - accuracy: 0.8087\n",
      "\n",
      "valid: 0.595\n",
      "test: 0.595\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 84ms/step - loss: 0.4870 - accuracy: 0.8087 - val_loss: 0.7519 - val_accuracy: 0.7162\n",
      "Epoch 107/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.8101\n",
      "Epoch 00107: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4844 - accuracy: 0.8101 - val_loss: 0.7572 - val_accuracy: 0.7137\n",
      "Epoch 108/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.8065\n",
      "Epoch 00108: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4887 - accuracy: 0.8065 - val_loss: 0.7615 - val_accuracy: 0.7137\n",
      "Epoch 109/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.8059\n",
      "Epoch 00109: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4866 - accuracy: 0.8059 - val_loss: 0.7578 - val_accuracy: 0.7151\n",
      "Epoch 110/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4846 - accuracy: 0.8093\n",
      "Epoch 00110: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4846 - accuracy: 0.8093 - val_loss: 0.7607 - val_accuracy: 0.7155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4874 - accuracy: 0.8087\n",
      "\n",
      "valid: 0.6075\n",
      "test: 0.6082\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.4874 - accuracy: 0.8087 - val_loss: 0.7577 - val_accuracy: 0.7140\n",
      "Epoch 112/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.8096\n",
      "Epoch 00112: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4849 - accuracy: 0.8096 - val_loss: 0.7583 - val_accuracy: 0.7168\n",
      "Epoch 113/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4854 - accuracy: 0.8096\n",
      "Epoch 00113: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4854 - accuracy: 0.8096 - val_loss: 0.7556 - val_accuracy: 0.7171\n",
      "Epoch 114/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.8084\n",
      "Epoch 00114: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4836 - accuracy: 0.8084 - val_loss: 0.7522 - val_accuracy: 0.7162\n",
      "Epoch 115/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4817 - accuracy: 0.8113\n",
      "Epoch 00115: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4817 - accuracy: 0.8113 - val_loss: 0.7597 - val_accuracy: 0.7173\n",
      "Epoch 116/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.8113\n",
      "\n",
      "valid: 0.6045\n",
      "test: 0.6023\n",
      "\n",
      "\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 25s 83ms/step - loss: 0.4839 - accuracy: 0.8113 - val_loss: 0.7570 - val_accuracy: 0.7158\n",
      "Epoch 117/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.8104\n",
      "Epoch 00117: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.4826 - accuracy: 0.8104 - val_loss: 0.7579 - val_accuracy: 0.7160\n",
      "Epoch 118/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8090\n",
      "Epoch 00118: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 80ms/step - loss: 0.4830 - accuracy: 0.8090 - val_loss: 0.7573 - val_accuracy: 0.7182\n",
      "Epoch 119/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4801 - accuracy: 0.8131\n",
      "Epoch 00119: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 81ms/step - loss: 0.4801 - accuracy: 0.8131 - val_loss: 0.7609 - val_accuracy: 0.7152\n",
      "Epoch 120/120\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4800 - accuracy: 0.8132\n",
      "Epoch 00120: val_loss did not improve from 0.74023\n",
      "297/297 [==============================] - 24s 82ms/step - loss: 0.4800 - accuracy: 0.8132 - val_loss: 0.7683 - val_accuracy: 0.7123\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        x=data_generator.flow(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            seed=RANDOM_SEED,\n",
    "        ),\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(x_hyper_train, y_hyper_train),\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_weights(BASE_MODEL_SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.557"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val acc\n",
    "preds_val = utils.utils.compute_preds(\n",
    "    model,\n",
    "    x_val_full,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "(np.argmax(preds_val, axis=1) == np.argwhere(y_val_full)[:,1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5588"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test acc\n",
    "preds_test = utils.utils.compute_preds(\n",
    "    model,\n",
    "    x_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "(np.argmax(preds_test, axis=1) == np.argwhere(y_test)[:,1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
