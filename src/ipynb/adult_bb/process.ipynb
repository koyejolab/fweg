{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import log_loss\n",
    "import sklearn\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import model_selection\n",
    "\n",
    "BASE_DIR = '../../../'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# custom code\n",
    "import utils.utils\n",
    "CONFIG = utils.utils.load_config(\"../../config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "EVAL_GROUPS: ['gender_Male', 'gender_Female']\n"
     ]
    }
   ],
   "source": [
    "DATASET = os.path.basename(os.getcwd()) # name of folder this file is in\n",
    "RANDOM_SEED = CONFIG['random_seed']\n",
    "EVAL_GROUPS = CONFIG['experiment_configs'][DATASET]['eval_groups']\n",
    "HYPER_TRAIN_SPLIT = CONFIG[\"experiment_configs\"][DATASET][\"hyper_train_split\"]\n",
    "VAL_FULL_SPLIT = CONFIG[\"experiment_configs\"][DATASET][\"val_full_split\"]\n",
    "\n",
    "print(RANDOM_SEED)\n",
    "print(f\"EVAL_GROUPS: {EVAL_GROUPS}\")\n",
    "\n",
    "DATA_F = os.path.join(BASE_DIR, f'data/adult/')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, f'processed/{DATASET}/rs={RANDOM_SEED}')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, f'models/{DATASET}/rs={RANDOM_SEED}')\n",
    "\n",
    "PROCESSED_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, DATASET, \".pkl\", eg=EVAL_GROUPS)\n",
    "\n",
    "# processing saved here\n",
    "if os.path.exists(PROCESSED_SAVEPATH):\n",
    "    print(f\"warning: processing has been done for rs={RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adult_data():\n",
    "\n",
    "    CATEGORICAL_COLUMNS = [\n",
    "      'workclass', 'education', 'occupation', 'relationship', \n",
    "      'race', 'gender', 'native_country'\n",
    "    ]\n",
    "    CONTINUOUS_COLUMNS = [\n",
    "      'age', 'capital_gain', 'capital_loss', 'hours_per_week', 'education_num'\n",
    "    ]\n",
    "    COLUMNS = [\n",
    "      'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "      'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "      'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "      'income_bracket'\n",
    "    ]\n",
    "    LABEL_COLUMN = 'label'\n",
    "\n",
    "    train_df_raw = pd.read_csv(\n",
    "        os.path.join( DATA_F, 'adult.data' ),\n",
    "        names=COLUMNS,\n",
    "        skipinitialspace=True,\n",
    "    )\n",
    "    test_df_raw = pd.read_csv(\n",
    "        os.path.join( DATA_F, 'adult.test' ),\n",
    "        names=COLUMNS,\n",
    "        skipinitialspace=True,\n",
    "        skiprows=1,\n",
    "    )\n",
    "\n",
    "    train_df_raw[LABEL_COLUMN] = (train_df_raw['income_bracket'].apply(\n",
    "        lambda x: '>50K' in x)).astype(int)\n",
    "    test_df_raw[LABEL_COLUMN] = (test_df_raw['income_bracket'].apply(\n",
    "        lambda x: '>50K' in x)).astype(int)\n",
    "\n",
    "    # Preprocessing Features\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # Functions for preprocessing categorical and continuous columns.\n",
    "    def binarize_categorical_columns(\n",
    "        input_train_df, input_test_df, categorical_columns=[]):\n",
    "\n",
    "        def fix_columns(input_train_df, input_test_df):\n",
    "            test_df_missing_cols = set(input_train_df.columns) - set(\n",
    "                input_test_df.columns)\n",
    "            for c in test_df_missing_cols:\n",
    "                input_test_df[c] = 0\n",
    "                train_df_missing_cols = set(input_test_df.columns) - set(\n",
    "                    input_train_df.columns)\n",
    "            for c in train_df_missing_cols:\n",
    "                input_train_df[c] = 0\n",
    "                input_train_df = input_train_df[input_test_df.columns]\n",
    "            return input_train_df, input_test_df\n",
    "\n",
    "        # Binarize categorical columns.\n",
    "        binarized_train_df = pd.get_dummies(\n",
    "            input_train_df, columns=categorical_columns)\n",
    "        binarized_test_df = pd.get_dummies(\n",
    "            input_test_df, columns=categorical_columns)\n",
    "        # Make sure the train and test dataframes have the same binarized columns.\n",
    "        fixed_train_df, fixed_test_df = fix_columns(\n",
    "            binarized_train_df, binarized_test_df)\n",
    "        return fixed_train_df, fixed_test_df\n",
    "\n",
    "    def bucketize_continuous_column(input_train_df,\n",
    "                                input_test_df,\n",
    "                                continuous_column_name,\n",
    "                                num_quantiles=None,\n",
    "                                bins=None):\n",
    "        assert (num_quantiles is None or bins is None)\n",
    "        if num_quantiles is not None:\n",
    "            train_quantized, bins_quantized = pd.qcut(\n",
    "                input_train_df[continuous_column_name],\n",
    "                num_quantiles,\n",
    "                retbins=True,\n",
    "                labels=False)\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "                input_train_df[continuous_column_name], bins_quantized, \n",
    "                labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "                input_test_df[continuous_column_name], bins_quantized, labels=False)\n",
    "        elif bins is not None:\n",
    "            input_train_df[continuous_column_name] = pd.cut(\n",
    "                input_train_df[continuous_column_name], bins, labels=False)\n",
    "            input_test_df[continuous_column_name] = pd.cut(\n",
    "                input_test_df[continuous_column_name], bins, labels=False)\n",
    "\n",
    "    # Filter out all columns except the ones specified.\n",
    "    train_df = (\n",
    "        train_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]])\n",
    "    test_df = (\n",
    "        test_df_raw[CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS + [LABEL_COLUMN]])\n",
    "  \n",
    "    # Bucketize continuous columns.\n",
    "    bucketize_continuous_column(train_df, test_df, 'age', num_quantiles=4)\n",
    "    bucketize_continuous_column(\n",
    "        train_df, test_df, 'capital_gain', bins=[-1, 1, 4000, 10000, 100000])\n",
    "    bucketize_continuous_column(\n",
    "        train_df, test_df, 'capital_loss', bins=[-1, 1, 1800, 1950, 4500])\n",
    "    bucketize_continuous_column(\n",
    "        train_df, test_df, 'hours_per_week', bins=[0, 39, 41, 50, 100])\n",
    "    bucketize_continuous_column(\n",
    "        train_df, test_df, 'education_num', bins=[0, 8, 9, 11, 16])\n",
    "  \n",
    "    train_df, test_df = binarize_categorical_columns(\n",
    "        train_df, test_df, \n",
    "        categorical_columns=CATEGORICAL_COLUMNS + CONTINUOUS_COLUMNS)\n",
    "\n",
    "    train_df[\"private_workforce\"] = train_df[\"workclass_Private\"]\n",
    "    train_df[\"non_private_workforce\"] = 1- train_df[\"workclass_Private\"]\n",
    "    test_df[\"private_workforce\"] = test_df[\"workclass_Private\"]\n",
    "    test_df[\"non_private_workforce\"] = 1 - test_df[\"workclass_Private\"]\n",
    "    \n",
    "    train_df.rename(columns={\"race_Amer-Indian-Eskimo\": \"race_Amer_Indian_Eskimo\", \n",
    "                             \"race_Asian-Pac-Islander\": \"race_Asian_Pac_Islander\"}, inplace = True)\n",
    "    test_df.rename(columns={\"race_Amer-Indian-Eskimo\": \"race_Amer_Indian_Eskimo\", \n",
    "                             \"race_Asian-Pac-Islander\": \"race_Asian_Pac_Islander\"}, inplace = True)\n",
    "    \n",
    "    cols = [c for c in train_df.columns if 'workclass' not in c]\n",
    "    train_df = train_df[cols]\n",
    "    test_df = test_df[cols]\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = get_adult_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split off train/hyper_train\n",
    "train_df, hyper_train_df = model_selection.train_test_split(\n",
    "    train_data,\n",
    "    test_size=HYPER_TRAIN_SPLIT,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=train_data['label'].values,\n",
    ")\n",
    "\n",
    "del train_data\n",
    "\n",
    "# split off val from test\n",
    "test_df, val_full_df = model_selection.train_test_split(\n",
    "    test_data,\n",
    "    test_size=VAL_FULL_SPLIT,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=test_data[EVAL_GROUPS + ['label']].values,\n",
    ")\n",
    "\n",
    "del test_data\n",
    "\n",
    "# we do not use a val/hyper_val in this setting so they are made the same\n",
    "val_df = val_full_df\n",
    "hyper_val_df = val_full_df\n",
    "\n",
    "del val_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21815, 109) (10746, 109) (1629, 109) (1629, 109) (14652, 109)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, hyper_train_df.shape, val_df.shape, hyper_val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(os.path.join(PROCESSED_DIR, \"train.csv\"), index=False)\n",
    "hyper_train_df.to_csv(os.path.join(PROCESSED_DIR, \"hyper_train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(PROCESSED_DIR, \"val.csv\"), index=False)\n",
    "hyper_val_df.to_csv(os.path.join(PROCESSED_DIR, \"hyper_val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(PROCESSED_DIR, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
