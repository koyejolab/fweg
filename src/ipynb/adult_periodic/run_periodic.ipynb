{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import functools\n",
    "\n",
    "BASE_DIR = '../../../'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# custom code\n",
    "import utils.utils\n",
    "CONFIG = utils.utils.load_config(\"../../config.json\")\n",
    "from utils.fweg import FWEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "DATASET = os.path.basename(os.getcwd()) # name of folder this file is in\n",
    "RANDOM_SEED = CONFIG['random_seed']\n",
    "BATCH_SIZE = CONFIG[\"experiment_configs\"][DATASET][\"batch_size\"]\n",
    "\n",
    "print(RANDOM_SEED)\n",
    "\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, f'processed/{DATASET}/rs={RANDOM_SEED}')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, f'models/{DATASET}/rs={RANDOM_SEED}')\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"train.csv\"))\n",
    "hyper_train_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"hyper_train.csv\"))\n",
    "val_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"val.csv\"))\n",
    "hyper_val_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"hyper_val.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(PROCESSED_DIR, \"test.csv\"))\n",
    "\n",
    "train_full_df = pd.concat([train_df, hyper_train_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full = train_full_df.drop('label', axis=1).values\n",
    "y_train_full = train_full_df['label'].values\n",
    "\n",
    "x_hyper_train = hyper_train_df.drop('label', axis=1).values\n",
    "y_hyper_train = hyper_train_df['label'].values\n",
    "\n",
    "x_val = val_df.drop('label', axis=1).values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "x_hyper_val = hyper_val_df.drop('label', axis=1).values\n",
    "y_hyper_val = hyper_val_df['label'].values\n",
    "\n",
    "x_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=x_train_full.shape[1]),\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(model, x, y):\n",
    "    preds = utils.utils.compute_preds(\n",
    "        model,\n",
    "        x,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    return float((np.argmax(preds, axis=1) == y).mean())\n",
    "\n",
    "def get_loss(model, x, y):\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "    preds = model.predict(x, batch_size=BATCH_SIZE)\n",
    "    return float(loss(y, preds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating base model\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating base model\")\n",
    "METRIC = 'Accuracy'\n",
    "batches = []\n",
    "hyper_train_acc = []\n",
    "val_acc = []\n",
    "hyper_val_acc = []\n",
    "test_acc = []\n",
    "hyper_train_loss = []\n",
    "val_loss = []\n",
    "hyper_val_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for batch in range(0, 1300, 100):\n",
    "    batches.append(batch)\n",
    "    model.load_weights( os.path.join(MODELS_DIR, f\"adult_periodic_base_batch={batch}.h5\") )\n",
    "    \n",
    "    hta = get_acc(model, x_hyper_train, y_hyper_train)\n",
    "    va = get_acc(model, x_val, y_val)\n",
    "    hva = get_acc(model, x_hyper_val, y_hyper_val)\n",
    "    ta = get_acc(model, x_test, y_test)\n",
    "    \n",
    "    htl = get_loss(model, x_hyper_train, y_hyper_train)\n",
    "    vl = get_loss(model, x_val, y_val)\n",
    "    hvl = get_loss(model, x_hyper_val, y_hyper_val)\n",
    "    tl = get_loss(model, x_test, y_test)\n",
    "    \n",
    "    hyper_train_acc.append(hta)\n",
    "    val_acc.append(va)\n",
    "    hyper_val_acc.append(hva)\n",
    "    test_acc.append(ta)\n",
    "    \n",
    "    hyper_train_loss.append(htl)\n",
    "    val_loss.append(vl)\n",
    "    hyper_val_loss.append(hvl)\n",
    "    test_loss.append(tl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results file exists, appending to it...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "savepath = os.path.join(RESULTS_DIR, f\"results_{DATASET}.csv\")\n",
    "saver = utils.record.Results_Recorder(savepath, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "for i in range(len(batches)):\n",
    "    extra = {\n",
    "        'batch': batches[i],\n",
    "        'train_acc': hyper_train_acc[i],\n",
    "        'train_loss': hyper_train_loss[i],\n",
    "        'val_loss': val_loss[i],\n",
    "        'test_loss': test_loss[i]\n",
    "    }\n",
    "    saver.save(\n",
    "        RANDOM_SEED,\n",
    "        METRIC,\n",
    "        \"base\",\n",
    "        val_acc[i],\n",
    "        hyper_val_acc[i],\n",
    "        test_acc[i],\n",
    "        json.dumps(extra)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis_fns(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    hyper_val_df,\n",
    "    test_df,\n",
    "    groups,\n",
    "    add_all\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Get the basis functions for adult.\n",
    "    \"\"\"\n",
    "    all_groups = [\n",
    "        [],\n",
    "        ['private_workforce', 'non_private_workforce'],\n",
    "        ['private_workforce', 'non_private_workforce', 'income']\n",
    "    ]\n",
    "    assert groups in all_groups, f\"got unexpected groups {groups}\"\n",
    "    if len(groups) == 0:\n",
    "        assert add_all is True\n",
    "        \n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    if len(groups) == 0:\n",
    "        basis_train = pd.DataFrame(np.ones(len(train_df)), columns=[\"All\"])\n",
    "        basis_val = pd.DataFrame(np.ones(len(val_df)), columns=[\"All\"])\n",
    "        basis_hyper_val = pd.DataFrame(np.ones(len(hyper_val_df)), columns=[\"All\"])\n",
    "        basis_test = pd.DataFrame(np.ones(len(test_df)), columns=[\"All\"])\n",
    "    else:\n",
    "        basis_train = train_df[groups].copy()\n",
    "        basis_val = val_df[groups].copy()\n",
    "        basis_hyper_val = hyper_val_df[groups].copy()\n",
    "        basis_test = test_df[groups].copy()\n",
    "    \n",
    "        if add_all:\n",
    "            basis_train['All'] = 1.0\n",
    "            basis_val['All'] = 1.0\n",
    "            basis_hyper_val['All'] = 1.0\n",
    "            basis_test['All'] = 1.0\n",
    "        \n",
    "    return basis_train, basis_val, basis_hyper_val, basis_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 2\n",
    "NUM_ITERS = 100\n",
    "METRIC = \"Accuracy\"\n",
    "\n",
    "groups_list = [\n",
    "    ['private_workforce', 'non_private_workforce', 'income']\n",
    "]\n",
    "groups_descr_list = [\n",
    "    [\"workforce, non-private workforce, income\",]\n",
    "]\n",
    "add_all_list = [False, True]\n",
    "epsilon_list = [0.0001, 0.001]\n",
    "use_linear_val_metric_list = [False]\n",
    "\n",
    "# this fills in most of the arguments for our basis function creator\n",
    "# it is missing the `groups` arg and `add_all`. FWEG_Hyperparameter_Search\n",
    "# is given basis_fn_generator and will fill these in as it iterates over\n",
    "# the hyperparameters.\n",
    "basis_fn_generator = functools.partial(\n",
    "    get_basis_fns,\n",
    "    train_df = train_full_df,\n",
    "    val_df = val_df,\n",
    "    hyper_val_df = hyper_val_df,\n",
    "    test_df = test_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_saver = utils.record.Mock_Recorder()\n",
    "fweg_hp_s = utils.fweg.FWEG_Hyperparameter_Search(\n",
    "    mock_saver,\n",
    "    CLASSES,\n",
    "    NUM_ITERS,\n",
    "    METRIC,\n",
    "    basis_fn_generator,\n",
    "    groups_list,\n",
    "    groups_descr_list,\n",
    "    add_all_list,\n",
    "    epsilon_list,\n",
    "    use_linear_val_metric_list,\n",
    "    RANDOM_SEED,\n",
    "    use_convergence=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fweg(model):\n",
    "    preds_train_full = model.predict(x_train_full)\n",
    "    preds_val = model.predict(x_val)\n",
    "    preds_hyper_val = model.predict(x_hyper_val)\n",
    "    preds_test = model.predict(x_test)\n",
    "    \n",
    "     # search for best params\n",
    "    (best_groups, best_add_all, best_epsilon, best_use_linear_val_metric) = fweg_hp_s.search(\n",
    "        preds_train_full,\n",
    "        y_train_full,\n",
    "        preds_val,\n",
    "        y_val,\n",
    "        preds_hyper_val,\n",
    "        y_hyper_val,\n",
    "        preds_test,\n",
    "        y_test,\n",
    "    )\n",
    "    \n",
    "    print(best_groups)\n",
    "    \n",
    "    # evaluate best params\n",
    "    basis_train_full, basis_val, basis_hyper_val, basis_test = get_basis_fns(\n",
    "        train_full_df,\n",
    "        val_df,\n",
    "        hyper_val_df,\n",
    "        test_df,\n",
    "        best_groups,\n",
    "        best_add_all\n",
    "    )\n",
    "    fweg = utils.fweg.FWEG(\n",
    "        METRIC,\n",
    "        NUM_ITERS,\n",
    "        best_epsilon,\n",
    "        CLASSES,\n",
    "        best_use_linear_val_metric,\n",
    "        RANDOM_SEED,\n",
    "    )\n",
    "    \n",
    "    val_train_list, grad_norm_list, cond_list = fweg.fit(\n",
    "        preds_train_full,\n",
    "        y_train_full,\n",
    "        basis_train_full,\n",
    "        preds_val,\n",
    "        y_val,\n",
    "        basis_val,\n",
    "    )\n",
    "    # apply to hyper val set\n",
    "    preds_hyper_val_list, mval_hyper_val_list = fweg.predict(\n",
    "        preds_hyper_val,\n",
    "        y_hyper_val,\n",
    "        basis_hyper_val,\n",
    "        deterministic=False,\n",
    "    )\n",
    "    start = len(mval_hyper_val_list)//2\n",
    "    best_idx = start + np.argmax(mval_hyper_val_list[start:])\n",
    "    # apply to test set\n",
    "    preds_test_list, mval_test_list = fweg.predict(\n",
    "        preds_test,\n",
    "        y_test,\n",
    "        basis_test,\n",
    "        deterministic=False,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'val_score': val_train_list[best_idx],\n",
    "        'hyper_val_score': mval_hyper_val_list[best_idx],\n",
    "        'test_score': mval_test_list[best_idx],\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating fweg models\")\n",
    "fweg_results = dict()\n",
    "\n",
    "for batch in range(0, 1300, 100):\n",
    "    model.load_weights( os.path.join(MODELS_DIR, f\"adult_periodic_base_batch={batch}.h5\") )\n",
    "    fweg_model_results = run_fweg(model)\n",
    "    fweg_results[batch] = fweg_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in fweg_results:\n",
    "    saver.save(\n",
    "        RANDOM_SEED,\n",
    "        METRIC,\n",
    "        \"fweg\",\n",
    "        fweg_results[batch]['val_score'],\n",
    "        fweg_results[batch]['hyper_val_score'],\n",
    "        fweg_results[batch]['test_score'],\n",
    "        json.dumps({'batch': batch}),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
