{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn import model_selection\n",
    "from tqdm.notebook import tqdm\n",
    "import umap\n",
    "import time\n",
    "import functools\n",
    "\n",
    "BASE_DIR = '../../../'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "# custom code\n",
    "import utils.utils\n",
    "CONFIG = utils.utils.load_config(\"../../config.json\")\n",
    "\n",
    "import utils.fweg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adience_ablation 55 0.5\n"
     ]
    }
   ],
   "source": [
    "DATASET = os.path.basename(os.getcwd()) # name of folder this file is in\n",
    "RANDOM_SEED = CONFIG['random_seed']\n",
    "EPOCHS = CONFIG[\"experiment_configs\"][DATASET][\"epochs\"]\n",
    "BATCH_SIZE = CONFIG[\"experiment_configs\"][DATASET][\"batch_size\"]\n",
    "IMAGE_X_SIZE = CONFIG[\"experiment_configs\"][DATASET][\"image_x_size\"]\n",
    "IMAGE_Y_SIZE = CONFIG[\"experiment_configs\"][DATASET][\"image_y_size\"]\n",
    "IMAGE_SIZE = (IMAGE_Y_SIZE, IMAGE_X_SIZE)\n",
    "VAL_FULL_SPLIT = CONFIG['experiment_configs'][DATASET]['val_full_split']\n",
    "HYPER_VAL_SPLIT = CONFIG['experiment_configs'][DATASET]['hyper_val_split']\n",
    "\n",
    "print(DATASET, RANDOM_SEED, VAL_FULL_SPLIT)\n",
    "\n",
    "# folders for processed, models\n",
    "DATA_F = os.path.join(BASE_DIR, f\"data/{DATASET}/\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, f'processed/{DATASET}/rs={RANDOM_SEED}/vs={VAL_FULL_SPLIT}')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, f'models/{DATASET}/rs={RANDOM_SEED}/vs={VAL_FULL_SPLIT}')\n",
    "\n",
    "BASE_MODEL_SAVEPATH = utils.utils.get_savepath(MODELS_DIR, DATASET, \".h5\", mt=\"base\") # mt = model_type\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, 'results')\n",
    "\n",
    "# base model saved here\n",
    "if not os.path.exists(BASE_MODEL_SAVEPATH):\n",
    "    print(f\"warning: no model has been run for rs={RANDOM_SEED}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9795 files belonging to 2 classes.\n",
      "Found 2449 files belonging to 2 classes.\n",
      "Found 996 files belonging to 2 classes.\n",
      "Found 996 files belonging to 2 classes.\n",
      "Found 1992 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(PROCESSED_DIR, \"train\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    follow_links=True,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "hyper_train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(PROCESSED_DIR, \"hyper_train\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    follow_links=True,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(PROCESSED_DIR, \"val\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    follow_links=True,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "hyper_val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(PROCESSED_DIR, \"hyper_val\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    follow_links=True,\n",
    "    shuffle = False,\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(PROCESSED_DIR, \"test\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    label_mode='categorical',\n",
    "    follow_links=True,\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This will standardize the pixel data\n",
    "'''\n",
    "def preprocess(imgs, labels):\n",
    "    # turn from <0..255> to <0..1>\n",
    "    imgs = imgs / 255.0\n",
    "    means = np.array( [0.5, 0.5, 0.5] )\n",
    "    stds = np.array( [0.5, 0.5, 0.5] )\n",
    "    imgs = (imgs - means) / stds\n",
    "    return imgs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after loading the data, this will efficiently preprocess it in real-time\n",
    "# this approach is 3x faster than `flow_from_directory`\n",
    "train_ds = train_ds.map(preprocess)\n",
    "hyper_train_ds = hyper_train_ds.map(preprocess)\n",
    "val_ds = val_ds.map(preprocess)\n",
    "hyper_val_ds = hyper_val_ds.map(preprocess)\n",
    "test_ds = test_ds.map(preprocess)\n",
    "\n",
    "train_full_ds = train_ds.concatenate(hyper_train_ds)\n",
    "del train_ds, hyper_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = utils.utils.load_sorted_df(PROCESSED_DIR, \"val\")\n",
    "\n",
    "# map strings to ints\n",
    "gender_dict = {'m':1, 'f':0}\n",
    "age_dict = {'(60, 100)' : 7, '(48, 53)' : 6, '(38, 43)' : 5, \n",
    "            '(25, 32)' : 4, '(15, 20)' : 3, '(8, 12)' : 2, '(4, 6)' : 1, '(0, 2)' : 0}\n",
    "\n",
    "val_df.replace({\"age\": age_dict}, inplace=True)\n",
    "val_df.replace({\"gender\": gender_dict}, inplace=True)\n",
    "\n",
    "age_val = val_df.age.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_FULL_EMB_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, \"x_train_full_emb\", \".npy\")\n",
    "X_VAL_EMB_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, \"x_val_emb\", \".npy\")\n",
    "X_HYPER_VAL_EMB_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, \"x_hyper_val_emb\", \".npy\")\n",
    "X_TEST_EMB_SAVEPATH = utils.utils.get_savepath(PROCESSED_DIR, \"x_test_emb\", \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(X_TRAIN_FULL_EMB_SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full_emb = np.load(\n",
    "    X_TRAIN_FULL_EMB_SAVEPATH,\n",
    ")\n",
    "\n",
    "x_val_emb = np.load(\n",
    "    X_VAL_EMB_SAVEPATH,\n",
    ")\n",
    "\n",
    "x_hyper_val_emb = np.load(\n",
    "    X_HYPER_VAL_EMB_SAVEPATH,\n",
    ")\n",
    "\n",
    "x_test_emb = np.load(\n",
    "    X_TEST_EMB_SAVEPATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.utils.make_resnet(\n",
    "    depth=2,\n",
    "    random_state=RANDOM_SEED,\n",
    "    input_shape=(*IMAGE_SIZE, 3),\n",
    "    nc=2,\n",
    ")\n",
    "\n",
    "model.load_weights(BASE_MODEL_SAVEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 384/384 [01:27<00:00,  4.40it/s]\n",
      "100%|██████████| 32/32 [00:07<00:00,  4.56it/s]\n",
      "100%|██████████| 32/32 [00:06<00:00,  4.69it/s]\n",
      "100%|██████████| 63/63 [00:13<00:00,  4.59it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_train_full, y_train_full = utils.utils.compute_preds(\n",
    "    model,\n",
    "    train_full_ds,\n",
    ")\n",
    "\n",
    "preds_val, y_val = utils.utils.compute_preds(\n",
    "    model,\n",
    "    val_ds,\n",
    ")\n",
    "\n",
    "preds_hyper_val, y_hyper_val = utils.utils.compute_preds(\n",
    "    model,\n",
    "    hyper_val_ds,\n",
    ")\n",
    "\n",
    "preds_test, y_test = utils.utils.compute_preds(\n",
    "    model,\n",
    "    test_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel\n",
    "kernel = RBF(2)\n",
    "\n",
    "# compute kernels with respect to x_val_emb\n",
    "# ex: if x_train_emb has shape (50,10) and x_val_emb has shape (20,10)\n",
    "# this will compute the kernel for each row in x_train_emb with respect to each row in x_val_emb\n",
    "# output: (50, 20). Row x, column y = kernel computation between x_train_emb[x], x_val_emb[y]\n",
    "kernel_train_full = kernel(x_train_full_emb, Y = x_val_emb) # x_train_emb\n",
    "kernel_val = kernel(x_val_emb, Y = x_val_emb)\n",
    "kernel_hyper_val = kernel(x_hyper_val_emb, Y = x_val_emb)\n",
    "kernel_test = kernel(x_test_emb, Y = x_val_emb)\n",
    "\n",
    "kernel_train_full = np.clip(kernel_train_full, 0.01, 0.99)\n",
    "kernel_val = np.clip(kernel_val, 0.01, 0.99)\n",
    "kernel_hyper_val = np.clip(kernel_hyper_val, 0.01, 0.99)\n",
    "kernel_test = np.clip(kernel_test, 0.01, 0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis_fns(\n",
    "    groups,\n",
    "    kernel_train,\n",
    "    kernel_val,\n",
    "    kernel_hyper_val,\n",
    "    kernel_test,\n",
    "    y_val,\n",
    "    age_val,\n",
    "    add_all,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Creates basis functions whose rows denote the membership value for a sample for that basis function.\n",
    "\n",
    "    Args:\n",
    "        groups: how many groups to make. If 0\n",
    "        kernel_train: see kernel computation comment\n",
    "        kernel_val: see kernel computation comment\n",
    "        kernel_hyper_val: see kernel computation comment\n",
    "        kernel_test: see kernel computation comment\n",
    "        y_val: 1-D array of labels on the val set that will be used for training FWEG.\n",
    "        age_val: 1-D array of age groups for the val set\n",
    "        groups: number of RBF kernels to use (these are centered at validaiton\n",
    "          points that are randomly selected). This is essentially how many groups to make.\n",
    "        add_all: True iff there should be a group with all samples.\n",
    "\n",
    "    Returns:\n",
    "        basis_train: membership of each train sample to each group\n",
    "        basis_val_train: membership of each val train sample to each group\n",
    "        basis_val_test: membership of each val test sample to each group\n",
    "        basis_test: membership of each test sample to each group\n",
    "    \"\"\"\n",
    "    assert groups in [1, 2, 3, 6]\n",
    "    if groups == 1:\n",
    "        assert add_all is False\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "\n",
    "    if groups == 1:\n",
    "        basis_train = pd.DataFrame(np.ones(len(kernel_train)), columns=[\"All\"])\n",
    "        basis_val = pd.DataFrame(np.ones(len(kernel_val)), columns=[\"All\"])\n",
    "        basis_hyper_val = pd.DataFrame(np.ones(len(kernel_hyper_val)), columns=[\"All\"])\n",
    "        basis_test = pd.DataFrame(np.ones(len(kernel_test)), columns=[\"All\"])\n",
    "    else:\n",
    "        # Computing RBF kernel matrix centred at each validation points\n",
    "        basis_train = pd.DataFrame()\n",
    "        basis_val = pd.DataFrame()\n",
    "        basis_hyper_val = pd.DataFrame()\n",
    "        basis_test = pd.DataFrame()\n",
    "        \n",
    "        if(groups == 2):\n",
    "            # use gender as the basis\n",
    "            for cc in range(CLASSES):\n",
    "                # randomly choose val samples for the classes\n",
    "                choices = np.where(y_val == cc)[0]\n",
    "                basis_train[f\"cc={cc}\"] = kernel_train[:, choices].mean(axis=1)\n",
    "                basis_val[f\"cc={cc}\"] = kernel_val[:, choices].mean(axis=1)\n",
    "                basis_hyper_val[f\"cc={cc}\"] = kernel_hyper_val[:, choices].mean(axis=1)\n",
    "                basis_test[f\"cc={cc}\"] = kernel_test[:, choices].mean(axis=1)\n",
    "        elif(groups == 3):\n",
    "            # use age as the basis\n",
    "            # age groups range from 5 to 8 on val/test\n",
    "            for ag in range(5, 8):\n",
    "                # randomly choose val samples for the ages\n",
    "                choices = np.where(age_val == ag)[0]\n",
    "                basis_train[f\"ag={ag}\"] = kernel_train[:, choices].mean(axis=1)\n",
    "                basis_val[f\"ag={ag}\"] = kernel_val[:, choices].mean(axis=1)\n",
    "                basis_hyper_val[f\"ag={ag}\"] = kernel_hyper_val[:, choices].mean(axis=1)\n",
    "                basis_test[f\"ag={ag}\"] = kernel_test[:, choices].mean(axis=1)\n",
    "        else:\n",
    "            # use age and gender as the basis\n",
    "            # age groups range from 5 to 8 on val/test\n",
    "            for ag in range(5, 8):\n",
    "                for cc in range(CLASSES):\n",
    "                    # randomly choose val samples for each (class, age) tuple\n",
    "                    choices = np.where((y_val == cc) & (age_val == ag))[0]\n",
    "                    basis_train[f\"ag={ag}_cc={cc}\"] = kernel_train[:, choices].mean(axis=1)\n",
    "                    basis_val[f\"ag={ag}_cc={cc}\"] = kernel_val[:, choices].mean(axis=1)\n",
    "                    basis_hyper_val[f\"ag={ag}_cc={cc}\"] = kernel_hyper_val[:, choices].mean(axis=1)\n",
    "                    basis_test[f\"ag={ag}_cc={cc}\"] = kernel_test[:, choices].mean(axis=1)\n",
    "    \n",
    "    if add_all:\n",
    "        basis_train['All'] = 1.0\n",
    "        basis_val['All'] = 1.0\n",
    "        basis_hyper_val['All'] = 1.0\n",
    "        basis_test['All'] = 1.0\n",
    "    \n",
    "    return basis_train, basis_val, basis_hyper_val, basis_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 2\n",
    "\n",
    "# number of groups to make\n",
    "NUM_GROUPS = 6\n",
    "\n",
    "# have a group with all samples in it\n",
    "ADD_ALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basis_train is just the kernel distance of all train samples\n",
    "# to a chosen validation sample in some basis. for example, \n",
    "# if NUM_RBF = 6 then each (age, gender) tuple gets an associated randomly chosen valid sample\n",
    "# that has that age and gender\n",
    "# basis_train will be the kernel of each train sample to each basis\n",
    "basis_train_full, basis_val, basis_hyper_val, basis_test = get_basis_fns(\n",
    "    NUM_GROUPS,\n",
    "    kernel_train_full,\n",
    "    kernel_val,\n",
    "    kernel_hyper_val,\n",
    "    kernel_test,\n",
    "    y_val,\n",
    "    age_val,\n",
    "    add_all = ADD_ALL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for fweg\n",
    "\n",
    "# epsilon perturbation\n",
    "EPSILON = 1e-4\n",
    "\n",
    "METRIC = \"Accuracy\" # ['Accuracy', 'F-measure', 'G-mean']\n",
    "\n",
    "USE_LINEAR_VAL_METRIC = True\n",
    "\n",
    "NUM_ITERS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.556:   4%|▍         | 2/50 [00:00<00:04,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 0.503: 100%|██████████| 50/50 [00:03<00:00, 13.59it/s]\n"
     ]
    }
   ],
   "source": [
    "fweg = utils.fweg.FWEG(\n",
    "    METRIC,\n",
    "    NUM_ITERS,\n",
    "    EPSILON,\n",
    "    CLASSES,\n",
    "    USE_LINEAR_VAL_METRIC,\n",
    "    RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_train_list, grad_norm_list, cond_list = fweg.fit(\n",
    "    preds_train_full,\n",
    "    y_train_full,\n",
    "    basis_train_full,\n",
    "    preds_val,\n",
    "    y_val,\n",
    "    basis_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper Val: 0.714859437751004\n"
     ]
    }
   ],
   "source": [
    "# apply to hyper val set\n",
    "preds_hyper_val_list, mval_hyper_val_list = fweg.predict(\n",
    "    preds_hyper_val,\n",
    "    y_hyper_val,\n",
    "    basis_hyper_val,\n",
    "    deterministic=False,\n",
    ")\n",
    "\n",
    "best_idx = np.argmax(mval_hyper_val_list)\n",
    "print(f\"Hyper Val: {mval_hyper_val_list[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.7073293172690762\n"
     ]
    }
   ],
   "source": [
    "# apply to test set\n",
    "preds_test_list, mval_test_list = fweg.predict(\n",
    "    preds_test,\n",
    "    y_test,\n",
    "    basis_test,\n",
    "    deterministic=False,\n",
    ")\n",
    "print(f\"Test: {mval_test_list[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below to try many hyperparams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results file exists, appending to it...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "savepath = os.path.join(RESULTS_DIR, f\"results_ablation_vs={VAL_FULL_SPLIT}.csv\")\n",
    "saver = utils.record.Results_Recorder(savepath, DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 2\n",
    "NUM_ITERS = 50\n",
    "METRIC = \"Accuracy\"\n",
    "\n",
    "groups_list = [1, 2, 3, 6]\n",
    "groups_descr_list = [\"no groups\", \"gender groups\", \"age groups\", \"gender and age groups\"]\n",
    "add_all_list = [False, True]\n",
    "epsilon_list = [0.0001, 0.001, 1.0]\n",
    "use_linear_val_metric_list = [False, True]\n",
    "\n",
    "# this fills in most of the arguments for our basis function creator\n",
    "# it is missing the `groups` arg and `add_all`. FWEG_Hyperparameter_Search\n",
    "# is given basis_fn_generator and will fill these in as it iterates over\n",
    "# the hyperparameters.\n",
    "basis_fn_generator = functools.partial(\n",
    "    get_basis_fns,\n",
    "    kernel_train = kernel_train_full,\n",
    "    kernel_val = kernel_val,\n",
    "    kernel_hyper_val = kernel_hyper_val,\n",
    "    kernel_test = kernel_test,\n",
    "    y_val = y_val,\n",
    "    age_val = age_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best hyper val: 0.8353, test: 0.8193:  60%|██████    | 29/48 [00:09<00:09,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best hyper val: 0.8353, test: 0.8193:  85%|████████▌ | 41/48 [00:22<00:07,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n",
      "UNEXPECTED - already had a trivial classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best hyper val: 0.8353, test: 0.8193: 100%|██████████| 48/48 [00:32<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "fweg_hp_s = \\\n",
    "    utils.fweg.FWEG_Hyperparameter_Search(\n",
    "    saver,\n",
    "    CLASSES,\n",
    "    NUM_ITERS,\n",
    "    METRIC,\n",
    "    basis_fn_generator,\n",
    "    groups_list,\n",
    "    groups_descr_list,\n",
    "    add_all_list,\n",
    "    epsilon_list,\n",
    "    use_linear_val_metric_list,\n",
    "    RANDOM_SEED,\n",
    ")\n",
    "\n",
    "(best_groups, best_add_all, best_epsilon, best_use_linear_val_metric) = fweg_hp_s.search(\n",
    "        preds_train_full,\n",
    "        y_train_full,\n",
    "        preds_val,\n",
    "        y_val,\n",
    "        preds_hyper_val,\n",
    "        y_hyper_val,\n",
    "        preds_test,\n",
    "        y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m56",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m56"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
